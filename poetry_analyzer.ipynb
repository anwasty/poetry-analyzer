{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8d5706e6-553c-4262-b9a0-3f033115a379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8aaafe79fb54c49874470bc95e068da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 19:47:07 INFO: Downloaded file to /Users/anwasty/stanza_resources/resources.json\n",
      "2025-05-27 19:47:07 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2025-05-27 19:47:10 INFO: File exists: /Users/anwasty/stanza_resources/ru/default.zip\n",
      "2025-05-27 19:47:14 INFO: Finished downloading models and saved to /Users/anwasty/stanza_resources\n",
      "2025-05-27 19:47:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bae476f16a94cacb8327b1e5c94de9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 19:47:14 INFO: Downloaded file to /Users/anwasty/stanza_resources/resources.json\n",
      "2025-05-27 19:47:17 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "| depparse  | syntagrus_charlm   |\n",
      "| ner       | wikiner            |\n",
      "==================================\n",
      "\n",
      "2025-05-27 19:47:17 INFO: Using device: cpu\n",
      "2025-05-27 19:47:17 INFO: Loading: tokenize\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-05-27 19:47:17 INFO: Loading: pos\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-05-27 19:47:17 INFO: Loading: lemma\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-05-27 19:47:17 INFO: Loading: depparse\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-05-27 19:47:17 INFO: Loading: ner\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-05-27 19:47:19 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline(\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a879d5ed-3eb0-440b-89a6-70a0b7306f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    with open(file=\"lemmas.dat\", mode='rb') as f:\n",
    "        lemmas = pickle.loads(f.read())\n",
    "    with open(file=\"wordforms.dat\", mode='rb') as f:\n",
    "        wordforms = pickle.loads(f.read())\n",
    "    return lemmas, wordforms\n",
    "\n",
    "\n",
    "\n",
    "def special_cases(text, wordforms):\n",
    "    \"\"\"\n",
    "    Обрабатывает слова с дефисами\n",
    "    \"\"\"\n",
    "    replacements = {}\n",
    "\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        key = word.lower()\n",
    "        if (\"-\" in word or \" \" in word) and key in wordforms and len(wordforms[key]) == 1:\n",
    "            accented = wordforms[key][0][\"accentuated\"]\n",
    "            simple_form = key.replace(\"-\", \"\").replace(\" \", \"\")\n",
    "            simple_form_cap = simple_form[0].upper() + simple_form[1:]\n",
    "\n",
    "            replacements[simple_form] = accented\n",
    "            replacements[simple_form_cap] = accented[0].upper() + accented[1:]\n",
    "\n",
    "            if word[0].isupper():\n",
    "                text = text.replace(word, simple_form_cap)\n",
    "            else:\n",
    "                text = text.replace(word, simple_form)\n",
    "\n",
    "    return text, replacements\n",
    "\n",
    "\n",
    "def get_historic(res, word):\n",
    "    \"\"\"\n",
    "    В устаревших словах проводит замену \"нь\" на \"ни\"\n",
    "    \"\"\"\n",
    "    if word.get(\"historical\") and word.get(\"original\"):\n",
    "        replace_index = res.rfind(\"ни\")\n",
    "        if (\n",
    "            replace_index != -1 and\n",
    "            len(res) - replace_index <= 4 and\n",
    "            \"нь\" in word[\"original\"]\n",
    "        ):\n",
    "            return res[:replace_index] + \"нь\" + res[replace_index + 2:]\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def compatible(interpretation, lemma, pos, morph, lemmas):\n",
    "    \"\"\"\n",
    "    Проверяет, соответствуют ли морфологические признаки, определённые моделью\n",
    "    признакам из словаря wordforms для нахождения единственной ударной формы\n",
    "    \"\"\"\n",
    "    if interpretation == \"canonical\":\n",
    "        return True\n",
    "\n",
    "    form_parts = interpretation.lower().replace(\"-\", \" \").replace(\"/\", \" \").split()\n",
    "    morph_feats = set(morph.split(\"|\"))\n",
    "\n",
    "    cases_number = [\n",
    "        ([\"plural\"], \"Number=Plur\"),\n",
    "        ([\"singular\"], \"Number=Sing\"),\n",
    "        ([\"nominative\"], \"Case=Nom\"),\n",
    "        ([\"genitive\"], \"Case=Gen\"),\n",
    "        ([\"dative\"], \"Case=Dat\"),\n",
    "        ([\"instrumental\"], \"Case=Ins\"),\n",
    "        ([\"prepositional\", \"locative\"], \"Case=Loc\"),  \n",
    "    ]\n",
    "\n",
    "    for interp_keys, morph_tag in cases_number:\n",
    "        if any(k in form_parts for k in interp_keys):\n",
    "            continue\n",
    "        if morph_tag in morph_feats:\n",
    "            return False\n",
    "\n",
    "    if \"Case=Acc\" in morph_feats and \"accusative\" not in form_parts:\n",
    "        if not (pos == \"ADJ\" and \"Animacy=Inan\" in morph_feats):\n",
    "            return False\n",
    "\n",
    "    tense = [\n",
    "        ([\"present\", \"future\"], \"Tense=Past\"),\n",
    "        ([\"past\", \"future\"], \"Tense=Pres\"),\n",
    "        ([\"past\", \"present\"], \"Tense=Fut\"),\n",
    "    ]\n",
    "\n",
    "    for interp_keys, morph_tag in tense:\n",
    "        if any(k in form_parts for k in interp_keys) and morph_tag in morph_feats:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def single_accentuation(interpretations):\n",
    "    if len(interpretations) == 0:\n",
    "        return None\n",
    "    res = interpretations[0][\"accentuated\"]\n",
    "    for i in range(1, len(interpretations)):\n",
    "        if interpretations[i][\"accentuated\"] != res:\n",
    "            return None\n",
    "    return res\n",
    "\n",
    "\n",
    "def accentuate_word(word, lemmas):\n",
    "    \"\"\"\n",
    "    Ставит ударение в слове на основе морфлогических признаков и лемме\n",
    "    \"\"\"\n",
    "    if word[\"is_punctuation\"] or (\"interpretations\" not in word):\n",
    "        return word[\"token\"]\n",
    "\n",
    "    else:\n",
    "        res = single_accentuation(word[\"interpretations\"])\n",
    "        if not (res is None):\n",
    "            res = get_historic(res, word)\n",
    "            return res\n",
    "        else:\n",
    "            compatible_interpretations = []\n",
    "            for i in range(len(word[\"interpretations\"])):\n",
    "                if compatible(word[\"interpretations\"][i][\"form\"], word[\"interpretations\"][i][\"lemma\"], word[\"pos\"], word[\"morph\"], lemmas):\n",
    "                    compatible_interpretations.append(word[\"interpretations\"][i])\n",
    "            res = single_accentuation(compatible_interpretations)\n",
    "            if not (res is None):\n",
    "                res = get_historic(res, word)\n",
    "                return res\n",
    "            else:\n",
    "                new_compatible_interpretations = []\n",
    "                for i in range(len(compatible_interpretations)):\n",
    "                    if compatible_interpretations[i][\"lemma\"] == word[\"lemma\"]:\n",
    "                        new_compatible_interpretations.append(compatible_interpretations[i])\n",
    "                res = single_accentuation(new_compatible_interpretations)\n",
    "                if not (res is None):\n",
    "                    res = get_historic(res, word)\n",
    "                    return res\n",
    "                else:\n",
    "                    return word[\"token\"]\n",
    "\n",
    "\n",
    "def tokenize(text, wordforms):\n",
    "    \"\"\"\n",
    "    Токенизирует строку и извлекает морфологические признаки с помощью Станзы\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    text, replacements = special_cases(text, wordforms)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.words:\n",
    "            token_text = token.text\n",
    "\n",
    "            word = {\n",
    "                \"token\": token_text,\n",
    "                \"start_char\": token.start_char,\n",
    "                \"end_char\": token.end_char,\n",
    "                \"line_text\": text,\n",
    "            }\n",
    "\n",
    "            if token.upos == \"PUNCT\":\n",
    "                word.update({\n",
    "                    \"is_punctuation\": True,\n",
    "                    \"whitespace\": \" \"\n",
    "                })\n",
    "            else:\n",
    "                original_token = replacements.get(token_text, token_text)\n",
    "                word.update({\n",
    "                    \"token\": original_token,\n",
    "                    \"morph\": token.feats or \"\",\n",
    "                    \"pos\": token.upos,\n",
    "                    \"lemma\": token.lemma,\n",
    "                    \"is_punctuation\": False,\n",
    "                    \"uppercase\": original_token.isupper(),\n",
    "                    \"capital_letter\": original_token[0].isupper(),\n",
    "                    \"whitespace\": \" \"\n",
    "                })\n",
    "                \n",
    "                if original_token in wordforms:\n",
    "                    word[\"interpretations\"] = wordforms[original_token]\n",
    "                elif original_token.lower() in wordforms:\n",
    "                    word[\"interpretations\"] = wordforms[original_token.lower()]\n",
    "                else:\n",
    "                    alt = token_text.replace(\"нь\", \"ни\")\n",
    "                    if alt in wordforms:\n",
    "                        word[\"interpretations\"] = wordforms[alt]\n",
    "                        word[\"token\"] = alt\n",
    "                        word[\"historical\"] = True\n",
    "                        word[\"original\"] = original_token\n",
    "                    elif alt.lower() in wordforms:\n",
    "                        word[\"interpretations\"] = wordforms[alt.lower()]\n",
    "                        word[\"token\"] = alt\n",
    "                        word[\"historical\"] = True\n",
    "                        word[\"original\"] = original_token\n",
    "\n",
    "            res.append(word)\n",
    "\n",
    "    return res, replacements\n",
    "\n",
    "\n",
    "\n",
    "tokenized_lines = []  \n",
    "\n",
    "def accentuate(text, wordforms, lemmas):\n",
    "    \"\"\"\n",
    "    Основная функция, разбивает текст на строки, расставляет ударения\n",
    "    и возвращает текст с ударениями\n",
    "    \"\"\"\n",
    "    lines = text.splitlines(keepends=True)\n",
    "    output = []\n",
    "    global tokenized_lines\n",
    "    tokenized_lines = [] \n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            output.append(line)\n",
    "            tokenized_lines.append([]) \n",
    "            continue\n",
    "\n",
    "        words, replacements = tokenize(line, wordforms)\n",
    "        tokenized_lines.append(words)\n",
    "\n",
    "        result_line = \"\"\n",
    "        for i, word in enumerate(words):\n",
    "            accented = accentuate_word(word, lemmas)\n",
    "\n",
    "            if word[\"token\"] in replacements:\n",
    "                accented = replacements[word[\"token\"]]\n",
    "\n",
    "            if word.get(\"capital_letter\"):\n",
    "                accented = accented.capitalize()\n",
    "            if word.get(\"uppercase\"):\n",
    "                accented = accented.upper()\n",
    "\n",
    "            if word[\"is_punctuation\"]:\n",
    "                result_line = result_line.rstrip() + accented\n",
    "            else:\n",
    "                result_line += accented\n",
    "\n",
    "            if i + 1 < len(words) and not words[i + 1][\"is_punctuation\"]:\n",
    "                result_line += \" \"\n",
    "\n",
    "        result_line = result_line.rstrip()\n",
    "        if line.endswith(\"\\n\"):\n",
    "            result_line += \"\\n\"\n",
    "        output.append(result_line)\n",
    "\n",
    "    result = \"\".join(output)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "lemmas, wordforms = load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "354f6ba8-09ef-4087-9328-565f1f47d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = \"аеёиоуыэюяАЕЁИОУЫЭЮЯ\"\n",
    "\n",
    "def count_vowels(word):\n",
    "    return sum(1 for ch in word.lower() if ch in vowels)\n",
    "\n",
    "def is_mono(word):\n",
    "    return count_vowels(word) == 1\n",
    "\n",
    "def is_stress(word):\n",
    "    return \"́\" in word or 'ё' in word.lower()\n",
    "\n",
    "def is_pos(pos, feats):\n",
    "    if pos in {\"NOUN\", \"ADJ\", \"ADV\", \"PROPN\", \"PRON\"}:\n",
    "        return True\n",
    "    if pos == \"VERB\" and \"Aux=Yes\" not in feats:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def stress_to_mono(text, tokenized_lines):\n",
    "    \"\"\"\n",
    "    Расставляет ударения в односложных словах\n",
    "    \"\"\"\n",
    "    lines = text.splitlines(keepends=True)\n",
    "    new_lines = []\n",
    "\n",
    "    for line, tokens in zip(lines, tokenized_lines):\n",
    "        if line.strip() == \"\":\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        mod_line = line\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.get(\"is_punctuation\"):\n",
    "                continue\n",
    "\n",
    "            word = token[\"token\"]\n",
    "            if is_stress(word):\n",
    "                continue\n",
    "\n",
    "            stripped = re.sub(r'[^\\wёЁа-яА-Я-]', '', word)\n",
    "            if not is_mono(stripped):\n",
    "                continue\n",
    "\n",
    "            pos = token.get(\"pos\", \"\")\n",
    "            feats = token.get(\"morph\", \"\")\n",
    "\n",
    "            if is_pos(pos, feats) or (\n",
    "                i + 1 < len(tokens) and tokens[i + 1].get(\"is_punctuation\")\n",
    "            ):\n",
    "                for j, ch in enumerate(word):\n",
    "                    if ch.lower() in vowels:\n",
    "                        stressed_word = word[:j+1] + \"́\" + word[j+1:]\n",
    "                        mod_line = mod_line.replace(word, stressed_word, 1)\n",
    "                        break\n",
    "\n",
    "        new_lines.append(mod_line)\n",
    "\n",
    "    return \"\".join(new_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5c761577-8637-459e-8b32-a920258187e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_rhythm(accented_text):\n",
    "    \"\"\"\n",
    "    Определяет ритмическую схему строк, где 1 - ударный, 0 - безударный\n",
    "    \"\"\"\n",
    "    lines = accented_text.splitlines() \n",
    "    result = []\n",
    "\n",
    "    for line in lines:\n",
    "        pattern = []\n",
    "        i = 0\n",
    "        while i < len(line):\n",
    "            letter = line[i]\n",
    "\n",
    "            if letter.lower() in vowels:\n",
    "                if letter.lower() == 'ё':\n",
    "                    is_stress = True\n",
    "                elif i + 1 < len(line) and line[i + 1] in [\"́\"]:\n",
    "                    is_stress = True\n",
    "                    i += 1\n",
    "                else:\n",
    "                    is_stress = False\n",
    "\n",
    "                pattern.append(1 if is_stress else 0)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        result.append(pattern)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "aebba92e-77ae-4e72-95c8-9d37d83ac104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_meter(patterns):\n",
    "    \"\"\"\n",
    "    Анализирует ритм строки, определяет метр и размер стихотворения\n",
    "    \"\"\"\n",
    "    meters = [\"ямб\", \"хорей\", \"дактиль\", \"амфибрахий\", \"анапест\"]\n",
    "    meter_syllable = {\n",
    "        \"ямб\": 2,\n",
    "        \"хорей\": 2,\n",
    "        \"дактиль\": 3,\n",
    "        \"амфибрахий\": 3,\n",
    "        \"анапест\": 3\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    meter_count = {m: 0 for m in meters}\n",
    "    meter_score = {m: 0.0 for m in meters}\n",
    "    counted_lines = 0\n",
    "    meter_for_line = []\n",
    "\n",
    "    for line_number, pattern in enumerate(patterns):\n",
    "        if not pattern:\n",
    "            continue  \n",
    "\n",
    "        n = len(pattern)\n",
    "        scores = {}\n",
    "\n",
    "        iamb_score = sum(\n",
    "            1 for i in range(n)\n",
    "            if (i % 2 == 1 and pattern[i] == 1) or (i % 2 == 0 and pattern[i] == 0)\n",
    "        )\n",
    "        scores[\"ямб\"] = round(iamb_score / n * 100, 1)\n",
    "\n",
    "        chorea_score = sum(\n",
    "            1 for i in range(n)\n",
    "            if (i % 2 == 0 and pattern[i] == 1) or (i % 2 == 1 and pattern[i] == 0)\n",
    "        )\n",
    "        scores[\"хорей\"] = round(chorea_score / n * 100, 1)\n",
    "\n",
    "        dactyl_score = 0\n",
    "        feet_count = 0\n",
    "        for i in range(0, n - 2, 3):\n",
    "            if pattern[i:i+3] == [1, 0, 0]:\n",
    "                dactyl_score += 1\n",
    "            feet_count += 1\n",
    "        scores[\"дактиль\"] = round(dactyl_score / max(1, feet_count) * 100, 1)\n",
    "\n",
    "        amphibrach_score = 0\n",
    "        feet_count = 0\n",
    "        for i in range(0, n - 2, 3):\n",
    "            if pattern[i:i+3] == [0, 1, 0]:\n",
    "                amphibrach_score += 1\n",
    "            feet_count += 1\n",
    "        scores[\"амфибрахий\"] = round(amphibrach_score / max(1, feet_count) * 100, 1)\n",
    "\n",
    "        anapest_score = 0\n",
    "        feet_count = 0\n",
    "        for i in range(0, n - 2, 3):\n",
    "            if pattern[i:i+3] == [0, 0, 1]:\n",
    "                anapest_score += 1\n",
    "            feet_count += 1\n",
    "        scores[\"анапест\"] = round(anapest_score / max(1, feet_count) * 100, 1)\n",
    "\n",
    "        best_meter = max(scores, key=scores.get)\n",
    "        meter_for_line.append((pattern, best_meter))\n",
    "\n",
    "        meter_count[best_meter] += 1\n",
    "        for m in meters:\n",
    "            meter_score[m] += scores[m]\n",
    "        counted_lines += 1\n",
    "\n",
    "        results.append({\n",
    "            \"строка\": line_number + 1,\n",
    "            \"размер\": best_meter,\n",
    "            \"оценки\": scores\n",
    "        })\n",
    "\n",
    "    if counted_lines == 0:\n",
    "        result_all = {\n",
    "            \"размер\": \"не определён\",\n",
    "            \"строки по метру\": {},\n",
    "            \"по среднему\": {},\n",
    "            \"лучший по среднему\": None\n",
    "        }\n",
    "    else:\n",
    "        average_scores = {\n",
    "            m: round(meter_score[m] / counted_lines, 1)\n",
    "            for m in meters\n",
    "        }\n",
    "        most_voted = max(meter_count, key=meter_count.get)\n",
    "        best_avg = max(average_scores, key=average_scores.get)\n",
    "\n",
    "        sylls_foot = meter_syllable[best_avg]\n",
    "        foot_position_dict = defaultdict(list)\n",
    "\n",
    "        lengths = [len(p) for p, m in meter_for_line if m == most_voted]\n",
    "        if lengths:\n",
    "            most_common_length, _ = Counter(lengths).most_common(1)[0]\n",
    "        else:\n",
    "            most_common_length = 0\n",
    "\n",
    "        filtered_patterns = [\n",
    "            p for p, m in meter_for_line\n",
    "            if m == most_voted and len(p) == most_common_length\n",
    "        ]\n",
    "\n",
    "        for pattern in filtered_patterns:\n",
    "            feet = [pattern[i:i+sylls_foot] for i in range(0, len(pattern), sylls_foot)]\n",
    "            for pos, foot in enumerate(feet, start=1):\n",
    "                foot_position_dict[pos].append(foot)\n",
    "\n",
    "        valid_positions = [\n",
    "            pos for pos, foot_list in foot_position_dict.items()\n",
    "            if any(1 in f for f in foot_list)\n",
    "        ]\n",
    "\n",
    "        final_result = f\"{len(valid_positions)}-стопный {best_avg}\"\n",
    "\n",
    "        result_all = {\n",
    "            \"размер\": final_result,\n",
    "            \"строки по метру\": meter_count,\n",
    "            \"по среднему\": average_scores,\n",
    "            \"лучший по среднему\": best_avg\n",
    "        }\n",
    "\n",
    "    return results, result_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4a909cf7-7b6e-4092-98da-425661decf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_vowels = {'е', 'ё', 'и', 'ю', 'я'}\n",
    "hard_vowels = {'а', 'о', 'у', 'э', 'ы'}\n",
    "all_vowels = soft_vowels.union(hard_vowels)\n",
    "\n",
    "always_voiced = {'м', 'н', 'л', 'р', 'й', 'в'}\n",
    "voiced_consonants = {'б', 'в', 'г', 'д', 'ж', 'з'}\n",
    "voiceless_consonants = {'п', 'ф', 'к', 'т', 'ш', 'с', 'ц', 'ч', 'щ', 'х'}\n",
    "all_consonants = voiced_consonants.union(voiceless_consonants).union(always_voiced)\n",
    "\n",
    "vowel_symbols = ['а', 'о', 'у', 'ы', 'э', 'е', 'ё', 'и', 'ю', 'я']\n",
    "consonant_symbols = [\n",
    "    'б', 'в', 'г', 'д', 'ж', 'з', 'п', 'ф', 'к', 'т', 'ш', 'с',\n",
    "    'ц', 'ч', 'щ', 'х', 'м', 'н', 'л', 'р', 'й'\n",
    "]\n",
    "\n",
    "j_map = {'ё': 'о', 'е': 'э', 'ю': 'у', 'я': 'а'}\n",
    "\n",
    "consonant_equivalents = {\n",
    "    'б': ['п'], 'в': ['ф'], 'г': ['к'],\n",
    "    'д': ['т'], 'ж': ['ш'], 'з': ['с'],\n",
    "    'ц': ['т', 'с'], 'щ': ['ш']\n",
    "}\n",
    "\n",
    "soft_equivalents = {\n",
    "    ('и', 'ы'), ('и', 'е'), ('и', 'я'), ('и', 'а'),\n",
    "    ('е', 'э'), ('ё', 'о')\n",
    "}\n",
    "\n",
    "vowel_ids = {symbol: line_number for line_number, symbol in enumerate(vowel_symbols)}\n",
    "consonant_ids = {symbol: line_number for line_number, symbol in enumerate(consonant_symbols)}\n",
    "\n",
    "def symbol_id(symbol):\n",
    "    if symbol in vowel_ids:\n",
    "        return vowel_ids[symbol]\n",
    "    elif symbol in consonant_ids:\n",
    "        return consonant_ids[symbol]\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "321c6b8c-6eb9-46e8-aeea-ca733edc0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phoneme:\n",
    "    def __init__(self, kind, symbol, accent=\"none\", voiced=None, palatalized=None):\n",
    "        self.kind = kind \n",
    "        self.symbol = symbol\n",
    "        self.id = symbol_id(symbol)\n",
    "        self.accent = accent\n",
    "        self.voiced = voiced\n",
    "        self.palatalized = palatalized\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.kind == 'vowel':\n",
    "            return f\"V({self.symbol}, id={self.id}, {self.accent})\"\n",
    "        return f\"C({self.symbol}, id={self.id}, voiced={self.voiced}, pal={self.palatalized})\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ebb6dc55-33c2-4b58-9aba-0ee830b8df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_phonetic_transcript(word: str):\n",
    "    \"\"\"\n",
    "    Преобразует слово в список фонем\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    letters = list(word)\n",
    "    i = 0\n",
    "\n",
    "    while i < len(letters):\n",
    "        letter = letters[i].lower()\n",
    "        accent = \"none\"\n",
    "\n",
    "        if i + 1 < len(letters) and letters[i + 1] == '́':\n",
    "            accent = 'main'\n",
    "            i += 1\n",
    "\n",
    "        if letter == 'ё':\n",
    "            result.append(Phoneme(\"vowel\", 'ё', accent='main'))\n",
    "\n",
    "        elif letter in vowel_ids:\n",
    "            result.append(Phoneme(\"vowel\", letter, accent))\n",
    "\n",
    "        elif letter in consonant_ids:\n",
    "            voiced = letter in voiced_consonants or letter in always_voiced\n",
    "            palatalized = (\n",
    "                i + 1 < len(letters) and (letters[i + 1] == 'ь' or letters[i + 1] in soft_vowels)\n",
    "            )\n",
    "            result.append(Phoneme(\"consonant\", letter, voiced=voiced, palatalized=palatalized))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def normalize_vowel(letter):\n",
    "    return j_map.get(letter, letter)\n",
    "\n",
    "def phoneme_distance(p1, p2):\n",
    "    \"\"\"\n",
    "    Оценивает расстояние междум двумя фонемами\n",
    "    \"\"\"\n",
    "    if p1.kind != p2.kind:\n",
    "        return 1.0\n",
    "\n",
    "    if p1.kind == \"vowel\":\n",
    "        letter1 = normalize_vowel(p1.symbol)\n",
    "        letter2 = normalize_vowel(p2.symbol)\n",
    "        if letter1 == letter2:\n",
    "            return 0.0\n",
    "        if (letter1, letter2) in soft_equivalents or (letter2, letter1) in soft_equivalents:\n",
    "            return 0.3 \n",
    "        return 0.7\n",
    "\n",
    "    if p1.kind == \"consonant\":\n",
    "        letter1 = p1.symbol\n",
    "        letter2 = p2.symbol\n",
    "        if letter1 == letter2:\n",
    "            return 0.0\n",
    "        if letter2 in consonant_equivalents.get(letter1, []) or letter1 in consonant_equivalents.get(letter2, []):\n",
    "            return 0.2\n",
    "        return 1.0\n",
    "\n",
    "    return 1.0\n",
    "\n",
    "def extract_rhyme_tail(phonemes):\n",
    "    \"\"\"\n",
    "    Определяет ударный хвост слова: от ударной гласной до конца слова\n",
    "    \"\"\"\n",
    "    for i in range(len(phonemes) - 1, -1, -1):\n",
    "        p = phonemes[i]\n",
    "        if p.kind == \"vowel\" and p.accent == \"main\":\n",
    "            return phonemes[i:]\n",
    "    return []  \n",
    "\n",
    "def phoneme_sequence_distance(tail1, tail2, allow_truncate=True):\n",
    "    \"\"\"\n",
    "    Вычисляет среднее расстояние между рифмующимися хвостами\n",
    "    \"\"\"\n",
    "    if not tail1 or not tail2:\n",
    "        return 1.0\n",
    "\n",
    "    if not allow_truncate and len(tail1) != len(tail2):\n",
    "        return 1.0\n",
    "\n",
    "    min_len = min(len(tail1), len(tail2))\n",
    "    total = 0.0\n",
    "\n",
    "    for i in range(1, min_len + 1):\n",
    "        p1 = tail1[-i]\n",
    "        p2 = tail2[-i]\n",
    "        total += phoneme_distance(p1, p2)\n",
    "\n",
    "    distance = total / min_len\n",
    "\n",
    "    length_cost = abs(len(tail1) - len(tail2)) * 0.1\n",
    "    return round(distance + length_cost, 3)\n",
    "\n",
    "def check_rhyme(word1, word2, threshold=0.4):\n",
    "    p1 = full_phonetic_transcript(word1)\n",
    "    p2 = full_phonetic_transcript(word2)\n",
    "\n",
    "    tail1 = extract_rhyme_tail(p1)\n",
    "    tail2 = extract_rhyme_tail(p2)\n",
    "\n",
    "    if not tail1 or not tail2:\n",
    "        return False\n",
    "\n",
    "    distance = phoneme_sequence_distance(tail1, tail2)\n",
    "    return distance <= threshold\n",
    "\n",
    "def get_rhyme_type(phonemes):\n",
    "    count = 0\n",
    "    for p in phonemes:\n",
    "        if p.kind == \"vowel\":\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return \"нет ударения\"\n",
    "    elif count == 1:\n",
    "        return \"мужская\"\n",
    "    elif count == 2:\n",
    "        return \"женская\"\n",
    "    elif count == 3:\n",
    "        return \"дактилическая\"\n",
    "    else:\n",
    "        return \"гипердактилическая\"\n",
    "\n",
    "def detect_rhyme_pattern(scheme):\n",
    "    if len(scheme) != 4 or '-' in scheme:\n",
    "        return \"неопределённая\"\n",
    "\n",
    "    a, b, c, d = scheme\n",
    "\n",
    "    def same(x, y): return x == y\n",
    "\n",
    "    if same(a, b) and same(c, d):\n",
    "        return \"парная\"\n",
    "    elif same(a, c) and same(b, d):\n",
    "        return \"перекрёстная\"\n",
    "    elif same(a, d) and same(b, c):\n",
    "        return \"кольцевая\"\n",
    "    elif all(x == a for x in scheme):\n",
    "        return \"сквозная\"\n",
    "    else:\n",
    "        return \"смешанная\"\n",
    "\n",
    "def build_rhyme_scheme(lines, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Определяет рифмовую схему\n",
    "    \"\"\"\n",
    "    endings = []\n",
    "    tails = []\n",
    "    valid_values = []\n",
    "\n",
    "    for line_number, line in enumerate(lines):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        last_word = line.strip().split()[-1]\n",
    "        phonemes = full_phonetic_transcript(last_word)\n",
    "        tail = extract_rhyme_tail(phonemes)\n",
    "        if not tail:\n",
    "            endings.append(None)\n",
    "            tails.append(None)\n",
    "            continue\n",
    "        endings.append(last_word)\n",
    "        tails.append(tail)\n",
    "        valid_values.append(line_number)\n",
    "\n",
    "    labels = {}\n",
    "    rhyme_labels = []\n",
    "    rhyme_types = []\n",
    "    next_label = 1\n",
    "\n",
    "    for i, tail1 in enumerate(tails):\n",
    "        if tail1 is None:\n",
    "            rhyme_labels.append(0) \n",
    "            rhyme_types.append('—')\n",
    "            continue\n",
    "\n",
    "        rhyme_found = False\n",
    "        for label, index in labels.items():\n",
    "            for j in index:\n",
    "                if check_rhyme(endings[i], endings[j], threshold=threshold):\n",
    "                    rhyme_labels.append(label)\n",
    "                    rhyme_types.append(get_rhyme_type(tail1))\n",
    "                    labels[label].append(i)\n",
    "                    rhyme_found = True\n",
    "                    break\n",
    "            if rhyme_found:\n",
    "                break\n",
    "\n",
    "        if not rhyme_found:\n",
    "            labels[next_label] = [i]\n",
    "            rhyme_labels.append(next_label)\n",
    "            rhyme_types.append(get_rhyme_type(tail1))\n",
    "            next_label += 1\n",
    "\n",
    "    return rhyme_labels, rhyme_types\n",
    "\n",
    "def describe_rhyme_scheme(scheme):\n",
    "    \"\"\"\n",
    "    Определяет тип рифмовки\n",
    "    \"\"\"\n",
    "    if not scheme or len(scheme) < 2:\n",
    "        return \"нет рифмы\"\n",
    "\n",
    "    normalized = [str(s) for s in scheme]\n",
    "    n = len(normalized)\n",
    "    description = set()\n",
    "    index_used = set()\n",
    "\n",
    "    i = 0\n",
    "    while i <= n - 4:\n",
    "        a, b, c, d = normalized[i:i+4]\n",
    "        if a == d and b == c and a != b:\n",
    "            description.add(\"кольцевая\")\n",
    "            index_used.update({i, i+1, i+2, i+3})\n",
    "            i += 4\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        if i in index_used or i + 1 in index_used:\n",
    "            continue\n",
    "        if normalized[i] == normalized[i + 1]:\n",
    "            description.add(\"смежная\")\n",
    "            index_used.update({i, i+1})\n",
    "\n",
    "    for i in range(n - 3):\n",
    "        if i in index_used and i+2 in index_used:\n",
    "            continue\n",
    "\n",
    "        is_cross = False\n",
    "        if normalized[i] == normalized[i + 2]:\n",
    "            is_cross = True\n",
    "            index_used.update({i, i+2})\n",
    "        if normalized[i + 1] == normalized[i + 3]:\n",
    "            is_cross = True\n",
    "            index_used.update({i+1, i+3})\n",
    "\n",
    "        if is_cross:\n",
    "            description.add(\"перекрёстная\")\n",
    "\n",
    "    if len(set(normalized)) == 1:\n",
    "        return \"сквозная\"\n",
    "\n",
    "    remaining = set(range(n)) - index_used\n",
    "    if description and remaining:\n",
    "        return \"смешанная: \" + \", \".join(description)\n",
    "    if not description:\n",
    "        return \"смешанная\"\n",
    "    if len(description) == 1:\n",
    "        return next(iter(description))\n",
    "    return \"смешанная: \" + \", \".join(description)\n",
    "\n",
    "def analyze_rhyme_structure(text_lines):\n",
    "    if isinstance(text_lines, str):\n",
    "        text_lines = text_lines.splitlines()\n",
    "\n",
    "    results = []\n",
    "    scheme_counter = Counter()\n",
    "\n",
    "    current_strofa = []\n",
    "    id_strofa = 1\n",
    "\n",
    "    for line in text_lines + [\"\"]: \n",
    "        if line.strip() == \"\":\n",
    "            if current_strofa:\n",
    "                scheme, rhyme_types = build_rhyme_scheme(current_strofa)\n",
    "                scheme_type = describe_rhyme_scheme(scheme)\n",
    "\n",
    "                results.append({\n",
    "                    \"строфа\": id_strofa,\n",
    "                    \"схема\": scheme,\n",
    "                    \"тип рифмовки\": scheme_type,\n",
    "                    \"рифмы по строкам\": rhyme_types,\n",
    "                    \"строки\": current_strofa\n",
    "                })\n",
    "                scheme_counter[scheme_type] += 1\n",
    "                id_strofa += 1\n",
    "                current_strofa = []\n",
    "        else:\n",
    "            current_strofa.append(line)\n",
    "\n",
    "    result_rhyming = {\n",
    "        \"рифмовка стихотворения\": scheme_counter.most_common(1)[0][0] if scheme_counter else \"нет рифмы\",\n",
    "        \"все рифмовки\": dict(scheme_counter)\n",
    "    }\n",
    "\n",
    "    return results, result_rhyming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72049e77-c06f-4d0b-80ab-3a9eab74fcdc",
   "metadata": {},
   "source": [
    "Ниже можно вставить своё стихотворение. Можете взять одно из выборки стихотворений: https://docs.google.com/spreadsheets/d/1_wLmEnxiYITVwixTEY0I3V24WuQT9yQm3Yb79WcpJM0/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ff267556-b885-4de2-9e1b-94f1f3d9e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вставьте своё стихотворение\n",
    "poem = \"\"\"\n",
    "По вечерам над ресторанами\n",
    "Горячий воздух дик и глух,\n",
    "И правит окриками пьяными\n",
    "Весенний и тлетворный дух.\n",
    "\n",
    "Вдали над пылью переулочной,\n",
    "Над скукой загородных дач,\n",
    "Чуть золотится крендель булочной,\n",
    "И раздаётся детский плач.\n",
    "\n",
    "И каждый вечер, за шлагбаумами,\n",
    "Заламывая котелки,\n",
    "Среди канав гуляют с дамами\n",
    "Испытанные остряки.\n",
    "\n",
    "Над озером скрипят уключины\n",
    "И раздается женский визг,\n",
    "А в небе, ко всему приученный\n",
    "Бессмысленно кривится диск.\n",
    "\n",
    "И каждый вечер друг единственный\n",
    "В моем стакане отражён\n",
    "И влагой терпкой и таинственной,\n",
    "Как я, смирён и оглушён.\n",
    "\n",
    "А рядом у соседних столиков\n",
    "Лакеи сонные торчат,\n",
    "И пьяницы с глазами кроликов\n",
    "«In vino veritas!»* кричат.\n",
    "\n",
    "И каждый вечер, в час назначенный\n",
    "(Иль это только снится мне?),\n",
    "Девичий стан, шелками схваченный,\n",
    "В туманном движется окне.\n",
    "\n",
    "И медленно, пройдя меж пьяными,\n",
    "Всегда без спутников, одна\n",
    "Дыша духами и туманами,\n",
    "Она садится у окна.\n",
    "\n",
    "И веют древними поверьями\n",
    "Ее упругие шелка,\n",
    "И шляпа с траурными перьями,\n",
    "И в кольцах узкая рука.\n",
    "\n",
    "И странной близостью закованный,\n",
    "Смотрю за темную вуаль,\n",
    "И вижу берег очарованный\n",
    "И очарованную даль.\n",
    "\n",
    "Глухие тайны мне поручены,\n",
    "Мне чьё-то солнце вручено,\n",
    "И все души моей излучины\n",
    "Пронзило терпкое вино.\n",
    "\n",
    "И перья страуса склонённые\n",
    "В моем качаются мозгу,\n",
    "И очи синие бездонные\n",
    "Цветут на дальнем берегу.\n",
    "\n",
    "В моей душе лежит сокровище,\n",
    "И ключ поручен только мне!\n",
    "Ты право, пьяное чудовище!\n",
    "Я знаю: истина в вине.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8de38f23-40df-4ca5-b566-b2fa73f077cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "По вечера́м над рестора́нами\n",
      "Горя́чий во́здух ди́к и глу́х,\n",
      "И пра́вит о́криками пья́ными\n",
      "Весе́нний и тлетво́рный ду́х.\n",
      "\n",
      "Вдали́ над пы́лью переу́лочной,\n",
      "Над ску́кой за́городных да́ч,\n",
      "Чу́ть золоти́тся кре́ндель бу́лочной,\n",
      "И раздаётся де́тский пла́ч.\n",
      "\n",
      "И ка́ждый ве́чер, за шлагба́умами,\n",
      "Зала́мывая котелки́,\n",
      "Среди́ кана́в гуля́ют с да́мами\n",
      "Испы́танные остряки́.\n",
      "\n",
      "Над о́зером скрипя́т уклю́чины\n",
      "И раздаётся же́нский ви́зг,\n",
      "А в не́бе, ко всему́ приу́ченный\n",
      "Бессмы́сленно криви́тся ди́ск.\n",
      "\n",
      "И ка́ждый ве́чер дру́г еди́нственный\n",
      "В моём стака́не отражён\n",
      "И вла́гой терпкой и таи́нственной,\n",
      "Как я́, смирён и оглушён.\n",
      "\n",
      "А ря́дом у сосе́дних сто́ликов\n",
      "Лаке́и со́нные торча́т,\n",
      "И пья́ницы с глаза́ми кро́ликов\n",
      "« In vino veritas!»* крича́т.\n",
      "\n",
      "И ка́ждый ве́чер, в ча́с назна́ченный\n",
      "( И́ль э́то то́лько сни́тся мне́?),\n",
      "Де́вичий ста́н, шелка́ми схва́ченный,\n",
      "В тума́нном дви́жется окне́.\n",
      "\n",
      "И ме́дленно, пройдя́ меж пья́ными,\n",
      "Всегда́ без спу́тников, одна́\n",
      "Дыша́ ду́хами и тума́нами,\n",
      "Она́ сади́тся у окна́.\n",
      "\n",
      "И ве́ют дре́вними пове́рьями\n",
      "Ее упру́гие шелка́,\n",
      "И шля́па с тра́урными пе́рьями,\n",
      "И в ко́льцах у́зкая рука́.\n",
      "\n",
      "И стра́нной бли́зостью зако́ванный,\n",
      "Смотрю́ за темную вуа́ль,\n",
      "И ви́жу берег очаро́ванный\n",
      "И очаро́ванную да́ль.\n",
      "\n",
      "Глухи́е та́йны мне́ пору́чены,\n",
      "Мне́ чьё-то со́лнце вручено́,\n",
      "И все ду́ши мое́й излу́чины\n",
      "Пронзи́ло терпкое вино́.\n",
      "\n",
      "И пе́рья стра́уса склонённые\n",
      "В моём кача́ются мо́згу,\n",
      "И о́чи си́ние бездо́нные\n",
      "Цвету́т на да́льнем берегу́.\n",
      "\n",
      "В мое́й душе́ лежи́т сокро́вище,\n",
      "И клю́ч пору́чен то́лько мне́!\n",
      "Ты́ пра́во, пья́ное чудо́вище!\n",
      "Я́ зна́ю: и́стина в вине.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Расстановка ударений\n",
    "res = accentuate(poem, wordforms, lemmas)\n",
    "res_fixed = stress_to_mono(res, tokenized_lines)\n",
    "print(res_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c16dc3cd-0f94-4d30-a29f-419c15f80558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 1, 0, 1]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0, 1, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0, 1, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[1, 1, 0, 1, 0, 1, 0, 1]\n",
      "[1, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[1, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 1, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[]\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 0, 1, 0, 1, 0, 1]\n",
      "[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[1, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Ритмическая схема стихотворения\n",
    "schemes = detect_rhythm(res_fixed)\n",
    "for i in schemes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0827ee8e-8fdc-4531-b3b6-fe408dc041c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-стопный ямб\n"
     ]
    }
   ],
   "source": [
    "# Размер стихотворения\n",
    "results, result_all = detect_meter(schemes)\n",
    "print(result_all[\"размер\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9e196593-c053-47ad-88d0-0b4d73a24e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ямб: 51 строк\n",
      "хорей: 0 строк\n",
      "дактиль: 0 строк\n",
      "амфибрахий: 0 строк\n",
      "анапест: 1 строк\n"
     ]
    }
   ],
   "source": [
    "# Количество строк, соответствующих метру стихотворения\n",
    "for meter, i in result_all[\"строки по метру\"].items():\n",
    "    print(f\"{meter}: {i} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "37c863d2-15ed-4236-91c0-63e9e6cff797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строка 2: размер = ямб, оценки = {'ямб': 70.0, 'хорей': 30.0, 'дактиль': 33.3, 'амфибрахий': 33.3, 'анапест': 0.0}\n",
      "Строка 3: размер = ямб, оценки = {'ямб': 100.0, 'хорей': 0.0, 'дактиль': 0.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 4: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 33.3, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 5: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 0.0, 'амфибрахий': 50.0, 'анапест': 50.0}\n",
      "Строка 7: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 33.3, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 8: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 9: размер = ямб, оценки = {'ямб': 70.0, 'хорей': 30.0, 'дактиль': 33.3, 'амфибрахий': 33.3, 'анапест': 0.0}\n",
      "Строка 10: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 0.0, 'амфибрахий': 0.0, 'анапест': 0.0}\n",
      "Строка 12: размер = ямб, оценки = {'ямб': 81.8, 'хорей': 18.2, 'дактиль': 33.3, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 13: размер = ямб, оценки = {'ямб': 75.0, 'хорей': 25.0, 'дактиль': 0.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 14: размер = ямб, оценки = {'ямб': 90.0, 'хорей': 10.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 15: размер = ямб, оценки = {'ямб': 75.0, 'хорей': 25.0, 'дактиль': 0.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 17: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 33.3}\n",
      "Строка 18: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 0.0, 'амфибрахий': 0.0, 'анапест': 0.0}\n",
      "Строка 19: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 33.3}\n",
      "Строка 20: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 0.0, 'амфибрахий': 50.0, 'анапест': 50.0}\n",
      "Строка 22: размер = ямб, оценки = {'ямб': 90.0, 'хорей': 10.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 23: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 24: размер = ямб, оценки = {'ямб': 70.0, 'хорей': 30.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 25: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 27: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 33.3}\n",
      "Строка 28: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 29: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 33.3}\n",
      "Строка 30: размер = ямб, оценки = {'ямб': 100.0, 'хорей': 0.0, 'дактиль': 0.0, 'амфибрахий': 0.0, 'анапест': 0.0}\n",
      "Строка 32: размер = ямб, оценки = {'ямб': 90.0, 'хорей': 10.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 33: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 0.0, 'амфибрахий': 0.0, 'анапест': 0.0}\n",
      "Строка 34: размер = ямб, оценки = {'ямб': 70.0, 'хорей': 30.0, 'дактиль': 33.3, 'амфибрахий': 33.3, 'анапест': 0.0}\n",
      "Строка 35: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 37: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 33.3}\n",
      "Строка 38: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 39: размер = ямб, оценки = {'ямб': 60.0, 'хорей': 40.0, 'дактиль': 0.0, 'амфибрахий': 33.3, 'анапест': 0.0}\n",
      "Строка 40: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 42: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 33.3, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 43: размер = ямб, оценки = {'ямб': 75.0, 'хорей': 25.0, 'дактиль': 50.0, 'амфибрахий': 0.0, 'анапест': 0.0}\n",
      "Строка 44: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 33.3, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 45: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 47: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 33.3, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 48: размер = ямб, оценки = {'ямб': 75.0, 'хорей': 25.0, 'дактиль': 0.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 49: размер = ямб, оценки = {'ямб': 70.0, 'хорей': 30.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 50: размер = ямб, оценки = {'ямб': 75.0, 'хорей': 25.0, 'дактиль': 50.0, 'амфибрахий': 0.0, 'анапест': 0.0}\n",
      "Строка 52: размер = ямб, оценки = {'ямб': 90.0, 'хорей': 10.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 53: размер = ямб, оценки = {'ямб': 75.0, 'хорей': 25.0, 'дактиль': 50.0, 'амфибрахий': 0.0, 'анапест': 0.0}\n",
      "Строка 54: размер = анапест, оценки = {'ямб': 60.0, 'хорей': 40.0, 'дактиль': 0.0, 'амфибрахий': 33.3, 'анапест': 66.7}\n",
      "Строка 55: размер = ямб, оценки = {'ямб': 75.0, 'хорей': 25.0, 'дактиль': 0.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 57: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 33.3, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 58: размер = ямб, оценки = {'ямб': 62.5, 'хорей': 37.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 59: размер = ямб, оценки = {'ямб': 80.0, 'хорей': 20.0, 'дактиль': 33.3, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 60: размер = ямб, оценки = {'ямб': 87.5, 'хорей': 12.5, 'дактиль': 50.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 62: размер = ямб, оценки = {'ямб': 90.0, 'хорей': 10.0, 'дактиль': 0.0, 'амфибрахий': 66.7, 'анапест': 0.0}\n",
      "Строка 63: размер = ямб, оценки = {'ямб': 100.0, 'хорей': 0.0, 'дактиль': 0.0, 'амфибрахий': 50.0, 'анапест': 0.0}\n",
      "Строка 64: размер = ямб, оценки = {'ямб': 70.0, 'хорей': 30.0, 'дактиль': 33.3, 'амфибрахий': 33.3, 'анапест': 0.0}\n",
      "Строка 65: размер = ямб, оценки = {'ямб': 62.5, 'хорей': 37.5, 'дактиль': 50.0, 'амфибрахий': 0.0, 'анапест': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Анализ по каждой строке\n",
    "for i in results:\n",
    "    print(f\"Строка {i['строка']}: размер = {i['размер']}, оценки = {i['оценки']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "5ec537ac-e908-4171-8ac0-853dff08364c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "перекрёстная\n"
     ]
    }
   ],
   "source": [
    "# Рифмовка стихотворения\n",
    "results, result_rhyming = analyze_rhyme_structure(res_fixed)\n",
    "print(result_rhyming[\"рифмовка стихотворения\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b16309b2-6c9c-45b8-ad14-549b0808c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строфа 1: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 2: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 3: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 4: смешанная: перекрёстная (схема: 1 2 3 2)\n",
      "Строфа 5: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 6: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 7: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 8: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 9: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 10: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 11: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 12: перекрёстная (схема: 1 2 1 2)\n",
      "Строфа 13: смешанная: перекрёстная (схема: 1 2 1 0)\n"
     ]
    }
   ],
   "source": [
    "# Рифмовка по каждой строфе\n",
    "for i in results:\n",
    "    print(f\"Строфа {i['строфа']}: {i['тип рифмовки']} (схема: {' '.join(map(str, i['схема']))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5c792dd1-06aa-4420-8b45-867c7b6fd821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чётные строки: мужская рифма\n",
      "Нечётные строки: дактилическая рифма\n"
     ]
    }
   ],
   "source": [
    "# Тип рифмы для строк\n",
    "rhymes_ev = []\n",
    "rhymes_od = []\n",
    "\n",
    "for i in results:\n",
    "    for i, rhyme_type in enumerate(i[\"рифмы по строкам\"]):\n",
    "        if rhyme_type != '—':\n",
    "            if i % 2 == 0:\n",
    "                rhymes_od.append(rhyme_type)\n",
    "            else:\n",
    "                rhymes_ev.append(rhyme_type)\n",
    "                \n",
    "for type, rhymes in [(\"Чётные\", rhymes_ev), (\"Нечётные\", rhymes_od)]:\n",
    "    result = Counter(rhymes).most_common(1)\n",
    "    if top:\n",
    "        print(f\"{type} строки: {result[0][0]} рифма\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa009d0-0b2e-4ce1-831a-8c8e589fed2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d7081-5482-4bc3-a61c-59c06a79dc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
